{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Kcs3s01DsHA",
    "outputId": "241ffca2-ac50-4f64-cacc-560e79035ad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 14.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 44.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 54.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=b7c8883d42486a028f7a9c4ab6182e8e8d409538862fcb3e9801b36d1dec3889\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !pip install transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForTokenClassification,AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqtnigZeD1Zr",
    "outputId": "2c0f3d71-b947-4b2e-e2b3-c0d96e3f3410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY6jQXuGDsHF"
   },
   "source": [
    "## Exploring the dataset\n",
    "The  dataset consists of negative and positive sentiments of movie reviews both in train and test set. The positive reviews are labelled as 1 and negative are labelled as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WXi9YovkDsHG"
   },
   "outputs": [],
   "source": [
    "# To create a dataset function.\n",
    "PATH = \"IMDB dataset/\"\n",
    "def get_data(args): #args are train or test\n",
    "    path = Path(os.path.join(PATH,args))\n",
    "    print(path)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_dir = [\"pos\",\"neg\"]\n",
    "    for text_files in (path/label_dir[0]).iterdir():\n",
    "        texts.append(text_files.read_text(encoding=\"utf8\"))\n",
    "        labels.append(1)\n",
    "    for text_files in (path/label_dir[1]).iterdir():\n",
    "        texts.append(text_files.read_text(encoding=\"utf8\"))\n",
    "        labels.append(0)\n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eM4E5h1zDsHG"
   },
   "outputs": [],
   "source": [
    "#It took long to run hence saved as pickle serialized object\n",
    "# train_texts,train_labels = get_data(\"train\")\n",
    "# test_texts,test_labels = get_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uj1_dlI7DsHH"
   },
   "outputs": [],
   "source": [
    "# len(train_texts),len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WS52f1MXDsHH"
   },
   "outputs": [],
   "source": [
    "#saving the list and then unpickling to access it\n",
    "def saving_lists(name,list_):\n",
    "    with open(f\"{name}.txt\",\"wb\") as fp:\n",
    "        pickle.dump(list_,fp)\n",
    "def unpickling(filename):\n",
    "    with open(filename,\"rb\") as fp:\n",
    "        file = pickle.load(fp)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hz6oLxVDsHI"
   },
   "outputs": [],
   "source": [
    "# saving_lists(\"Train_texts\",train_texts)\n",
    "# saving_lists(\"Test_texts\",test_texts)\n",
    "# saving_lists(\"Train_labels\",train_labels)\n",
    "# saving_lists(\"Test_labels\",test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z6YBxjHZEJpQ"
   },
   "outputs": [],
   "source": [
    "train_texts = unpickling(\"/content/drive/MyDrive/Train_texts.txt\")\n",
    "train_labels = unpickling(\"/content/drive/MyDrive/Train_labels.txt\")\n",
    "test_texts = unpickling(\"/content/drive/MyDrive/Test_texts.txt\")\n",
    "test_labels = unpickling(\"/content/drive/MyDrive/Test_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dT6ByUdcDsHI"
   },
   "outputs": [],
   "source": [
    "#splitting into train and validation data with test size of 20%\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "9b590b60be3b4e95ac131de65c8d7fcf",
      "47ae59bf5ebc499fb0d0b2ee482fd127",
      "bda1bf493f1d4e83aa3370a3bf669e23",
      "21e79ac10033433ca9ba0b34a3b5abb9",
      "321e150fa1864f5082f49ffd382a4a30",
      "ac68c99a46db410bbd1d3fb0843b508f",
      "72a3774e91ac464ba3b205f3fa4cb787",
      "d77dcf0be0914312909836ea39070a10",
      "d52d875d83c54a94af6f768a571f494a",
      "3281b1e48c5b4169818a988778aae2b8",
      "501824dd790a44349b8becc22a8f9126",
      "9780853cf5744ce18de54c277f59d070",
      "740db8cf72894bbdb6ceea132672281f",
      "fa534f9a353a4d09a1f05bf40add720c",
      "b631627a93154b7c8665c2f2fad027d8",
      "7f099d82a32e4b90bc154f852c9d91c0",
      "a6507069ab6d4c799102510221b871dc",
      "56a9b47951004cd5aa32dd7f9e390ddc",
      "329c33acd8d344e3abe99ffa585e41d2",
      "ad2c659fc02846e2ae62913990e8d859",
      "9b512d12fad44f638ec88056aa5c560a",
      "32b44b2e83b0409187257b8986b7434f",
      "1fbfecf68dba4c25ab0a2d0b73d5170c",
      "8c40a135259948448f599851ee090bf3"
     ]
    },
    "id": "MCT4LhvWDsHI",
    "outputId": "c31fd330-f7a4-4c2e-c923-e1bbc63ecea9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b590b60be3b4e95ac131de65c8d7fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52d875d83c54a94af6f768a571f494a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6507069ab6d4c799102510221b871dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Distilled Bert is a cut down version of Bert Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aG9st6e3DsHI"
   },
   "outputs": [],
   "source": [
    "#Ensure same maximum length of encodings\n",
    "#Encodings for train, validation and test dataset\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UWzkIK_FDsHJ"
   },
   "outputs": [],
   "source": [
    "#Source Hugging face https://huggingface.co/transformers/custom_datasets.html#seq-imdb\n",
    "#IMDB dataset to encodings and labels \n",
    "class IMDBdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,encodings,labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self,idx):\n",
    "        item = {key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LzyZJnkHDsHJ"
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBdataset(train_encodings,train_labels)\n",
    "test_dataset = IMDBdataset(test_encodings,test_labels)\n",
    "val_dataset = IMDBdataset(val_encodings,val_labels)\n",
    "\n",
    "#Dataloader functions\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a51ZP_dADsHJ"
   },
   "outputs": [],
   "source": [
    "#Using GPU device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Rh9paiVZ7irT"
   },
   "outputs": [],
   "source": [
    "#Initialize tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"/content/drive/MyDrive/runs_final/BERT_IMDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "hMsPALBlDsHK"
   },
   "outputs": [],
   "source": [
    "# Defining the last layers of Bert Model which would be trained to fit our specific purpose\n",
    "from transformers import BertModel\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self,freeze_bert=False):\n",
    "\n",
    "        super(BertClassifier,self).__init__()\n",
    "        D_in,H,D_out = 768,50,2\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(D_in,H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H,D_out)\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        def forward(self,input_ids,attention_mask):\n",
    "        outputs = self.bert(input_ids,attention_mask)\n",
    "        last_hidden_cls = outputs[0][:,0,:]\n",
    "        logits = self.classifier(last_hidden_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hAzgWqLOKUZM"
   },
   "outputs": [],
   "source": [
    "#Initialize optimizer and scheduler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "HiywaQdbmhHT"
   },
   "outputs": [],
   "source": [
    "#Training loop\n",
    "import random\n",
    "import time\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_loader, val_loader=None, epochs=4, evaluation=False):\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch_counts +=1\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_attn_mask = batch[\"attention_mask\"].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            losses.append(batch_loss)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            #Callbacks\n",
    "            if batch_counts > 200:\n",
    "                if (losses[step]>losses[step-1]) and (losses[step]>losses[step-2]) and (losses[step]>losses[step-3]):\n",
    "                    break\n",
    "\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_loader) - 1):\n",
    "      \n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                writer.add_scalar('At training step the loss ', step,round(batch_loss/batch_counts,4))\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "    torch.save(model.state_dict(),\"/content/drive/MyDrive/BERTModel.pth\")\n",
    "    return model,avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "-QL_FHpTm733"
   },
   "outputs": [],
   "source": [
    "#Evaluation loop\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    f1_scores = []\n",
    "    for idx,batch in enumerate(val_loader):\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attn_mask = batch[\"attention_mask\"].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "        score = f1_score(b_labels.cpu().numpy(),preds.cpu().numpy())\n",
    "        f1_scores.append(score) \n",
    "        if idx == 20:\n",
    "            writer.add_scalar('For step validation accuracy is  and loss respectively are ', idx, accuracy,round(loss.item(),2))\n",
    "            writer.add_scalar(\"The f1 score is \", score)\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    F1_score = np.mean(f1_scores)\n",
    "\n",
    "    return val_loss, val_accuracy,F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_N9WSfKtnMI9",
    "outputId": "37910ea2-344e-424a-b5be-5e8c4f8c239e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.739524   |   8.90   \n",
      "   1    |   40    |   0.714741   |   8.48   \n",
      "   1    |   60    |   0.709170   |   8.59   \n",
      "   1    |   80    |   0.748598   |   8.54   \n",
      "   1    |   100   |   0.720216   |   8.51   \n",
      "   1    |   120   |   0.700601   |   8.46   \n",
      "   1    |   140   |   0.705310   |   8.41   \n",
      "   1    |   160   |   0.714570   |   8.38   \n",
      "   1    |   180   |   0.683978   |   8.36   \n",
      "   1    |   200   |   0.686814   |   8.37   \n",
      "   1    |   220   |   0.693333   |   8.41   \n",
      "   1    |   240   |   0.723810   |   8.46   \n",
      "   1    |   260   |   0.703569   |   8.46   \n",
      "   1    |   280   |   0.691303   |   8.43   \n",
      "   1    |   300   |   0.699373   |   8.44   \n",
      "   1    |   320   |   0.718428   |   8.41   \n",
      "   1    |   340   |   0.692792   |   8.43   \n",
      "   1    |   360   |   0.700358   |   8.38   \n",
      "   1    |   380   |   0.693604   |   8.38   \n",
      "   1    |   400   |   0.698799   |   8.41   \n",
      "   1    |   420   |   0.706721   |   8.41   \n",
      "   1    |   440   |   0.694348   |   8.42   \n",
      "   1    |   460   |   0.739888   |   8.41   \n",
      "   1    |   480   |   0.695037   |   8.41   \n",
      "   1    |   500   |   0.723953   |   8.41   \n",
      "   1    |   520   |   0.703224   |   8.41   \n",
      "   1    |   540   |   0.705713   |   8.40   \n",
      "   1    |   560   |   0.699398   |   8.45   \n",
      "   1    |   580   |   0.697213   |   8.42   \n",
      "   1    |   600   |   0.700287   |   8.42   \n",
      "   1    |   620   |   0.699266   |   8.40   \n",
      "   1    |   640   |   0.688934   |   8.46   \n",
      "   1    |   660   |   0.707253   |   8.46   \n",
      "   1    |   680   |   0.703114   |   8.45   \n",
      "   1    |   700   |   0.701979   |   8.44   \n",
      "   1    |   720   |   0.668357   |   8.44   \n",
      "   1    |   740   |   0.747897   |   8.45   \n",
      "   1    |   760   |   0.703177   |   8.40   \n",
      "   1    |   780   |   0.689371   |   8.39   \n",
      "   1    |   800   |   0.711610   |   8.41   \n",
      "   1    |   820   |   0.698713   |   8.45   \n",
      "   1    |   840   |   0.685637   |   8.42   \n",
      "   1    |   860   |   0.692978   |   8.41   \n",
      "   1    |   880   |   0.705627   |   8.41   \n",
      "   1    |   900   |   0.692986   |   8.38   \n",
      "   1    |   920   |   0.697908   |   8.45   \n",
      "   1    |   940   |   0.675035   |   8.43   \n",
      "   1    |   960   |   0.770241   |   8.40   \n",
      "   1    |   980   |   0.695030   |   8.43   \n",
      "   1    |  1000   |   0.702300   |   8.41   \n",
      "   1    |  1020   |   0.742596   |   8.43   \n",
      "   1    |  1040   |   0.738864   |   8.45   \n",
      "   1    |  1060   |   0.693986   |   8.40   \n",
      "   1    |  1080   |   0.700798   |   8.41   \n",
      "   1    |  1100   |   0.696327   |   8.38   \n",
      "   1    |  1120   |   0.716904   |   8.40   \n",
      "   1    |  1140   |   0.697854   |   8.39   \n",
      "   1    |  1160   |   0.697184   |   8.43   \n",
      "   1    |  1180   |   0.691661   |   8.39   \n",
      "   1    |  1200   |   0.680513   |   8.35   \n",
      "   1    |  1220   |   0.697734   |   8.44   \n",
      "   1    |  1240   |   0.714269   |   8.40   \n",
      "   1    |  1260   |   0.696823   |   8.38   \n",
      "   1    |  1280   |   0.677764   |   8.38   \n",
      "   1    |  1300   |   0.706724   |   8.39   \n",
      "   1    |  1320   |   0.694904   |   8.43   \n",
      "   1    |  1340   |   0.694988   |   8.45   \n",
      "   1    |  1360   |   0.716205   |   8.43   \n",
      "   1    |  1380   |   0.698630   |   8.39   \n",
      "   1    |  1400   |   0.691807   |   8.40   \n",
      "   1    |  1420   |   0.699371   |   8.41   \n",
      "   1    |  1440   |   0.690689   |   8.39   \n",
      "   1    |  1460   |   0.706195   |   8.41   \n",
      "   1    |  1480   |   0.692606   |   8.43   \n",
      "   1    |  1500   |   0.701141   |   8.39   \n",
      "   1    |  1520   |   0.696223   |   8.39   \n",
      "   1    |  1540   |   0.708557   |   8.42   \n",
      "   1    |  1560   |   0.703348   |   8.38   \n",
      "   1    |  1580   |   0.694641   |   8.39   \n",
      "   1    |  1600   |   0.696656   |   8.38   \n",
      "   1    |  1620   |   0.687372   |   8.40   \n",
      "   1    |  1640   |   0.698835   |   8.38   \n",
      "   1    |  1660   |   0.695854   |   8.37   \n",
      "   1    |  1680   |   0.699847   |   8.42   \n",
      "   1    |  1700   |   0.692656   |   8.38   \n",
      "   1    |  1720   |   0.701702   |   8.38   \n",
      "   1    |  1740   |   0.690647   |   8.37   \n",
      "   1    |  1760   |   0.695668   |   8.40   \n",
      "   1    |  1780   |   0.703764   |   8.42   \n",
      "   1    |  1800   |   0.694064   |   8.39   \n",
      "   1    |  1820   |   0.688065   |   8.39   \n",
      "   1    |  1840   |   0.703246   |   8.41   \n",
      "   1    |  1860   |   0.667976   |   8.44   \n",
      "   1    |  1880   |   0.706921   |   8.35   \n",
      "   1    |  1900   |   0.693141   |   8.40   \n",
      "   1    |  1920   |   0.705933   |   8.43   \n",
      "   1    |  1940   |   0.692370   |   8.40   \n",
      "   1    |  1960   |   0.698302   |   8.43   \n",
      "   1    |  1980   |   0.671008   |   8.40   \n",
      "   1    |  2000   |   0.689430   |   8.41   \n",
      "   1    |  2020   |   0.699113   |   8.43   \n",
      "   1    |  2040   |   0.697287   |   8.39   \n",
      "   1    |  2060   |   0.694356   |   8.38   \n",
      "   1    |  2080   |   0.696601   |   8.43   \n",
      "   1    |  2100   |   0.695490   |   8.37   \n",
      "   1    |  2120   |   0.693308   |   8.41   \n",
      "   1    |  2140   |   0.699561   |   8.35   \n",
      "   1    |  2160   |   0.700594   |   8.40   \n",
      "   1    |  2180   |   0.694551   |   8.39   \n",
      "   1    |  2200   |   0.701637   |   8.41   \n",
      "   1    |  2220   |   0.698412   |   8.42   \n",
      "   1    |  2240   |   0.693767   |   8.41   \n",
      "   1    |  2260   |   0.690333   |   8.40   \n",
      "   1    |  2280   |   0.701586   |   8.42   \n",
      "   1    |  2300   |   0.692759   |   8.42   \n",
      "   1    |  2320   |   0.689203   |   8.38   \n",
      "   1    |  2340   |   0.696567   |   8.42   \n",
      "   1    |  2360   |   0.687254   |   8.39   \n",
      "   1    |  2380   |   0.705115   |   8.40   \n",
      "   1    |  2400   |   0.700504   |   8.39   \n",
      "   1    |  2420   |   0.697293   |   8.41   \n",
      "   1    |  2440   |   0.691927   |   8.37   \n",
      "   1    |  2460   |   0.687506   |   8.45   \n",
      "   1    |  2480   |   0.716593   |   8.40   \n",
      "   1    |  2500   |   0.702168   |   8.39   \n",
      "   1    |  2520   |   0.693161   |   8.44   \n",
      "   1    |  2540   |   0.696245   |   8.41   \n",
      "   1    |  2560   |   0.704008   |   8.40   \n",
      "   1    |  2580   |   0.692892   |   8.41   \n",
      "   1    |  2600   |   0.699471   |   8.40   \n",
      "   1    |  2620   |   0.690957   |   8.41   \n",
      "   1    |  2640   |   0.699768   |   8.38   \n",
      "   1    |  2660   |   0.692604   |   8.43   \n",
      "   1    |  2680   |   0.696298   |   8.39   \n",
      "   1    |  2700   |   0.690961   |   8.41   \n",
      "   1    |  2720   |   0.693487   |   8.39   \n",
      "   1    |  2740   |   0.698666   |   8.39   \n",
      "   1    |  2760   |   0.695897   |   8.41   \n",
      "   1    |  2780   |   0.693648   |   8.42   \n",
      "   1    |  2800   |   0.694109   |   8.38   \n",
      "   1    |  2820   |   0.693025   |   8.39   \n",
      "   1    |  2840   |   0.699539   |   8.43   \n",
      "   1    |  2860   |   0.694936   |   8.40   \n",
      "   1    |  2880   |   0.694966   |   8.39   \n",
      "   1    |  2900   |   0.692363   |   8.39   \n",
      "   1    |  2920   |   0.694661   |   8.40   \n",
      "   1    |  2940   |   0.697038   |   8.42   \n",
      "   1    |  2960   |   0.685843   |   8.38   \n",
      "   1    |  2980   |   0.698186   |   8.42   \n",
      "   1    |  3000   |   0.696875   |   8.39   \n",
      "   1    |  3020   |   0.693273   |   8.41   \n",
      "   1    |  3040   |   0.688176   |   8.40   \n",
      "   1    |  3060   |   0.702614   |   8.39   \n",
      "   1    |  3080   |   0.686214   |   8.41   \n",
      "   1    |  3100   |   0.695238   |   8.42   \n",
      "   1    |  3120   |   0.707974   |   8.42   \n",
      "   1    |  3140   |   0.692453   |   8.37   \n",
      "   1    |  3160   |   0.699626   |   8.43   \n",
      "   1    |  3180   |   0.694518   |   8.43   \n",
      "   1    |  3200   |   0.698919   |   8.39   \n",
      "   1    |  3220   |   0.694790   |   8.42   \n",
      "   1    |  3240   |   0.695835   |   8.41   \n",
      "   1    |  3260   |   0.695513   |   8.43   \n",
      "   1    |  3280   |   0.691919   |   8.43   \n",
      "   1    |  3300   |   0.696935   |   8.37   \n",
      "   1    |  3320   |   0.703247   |   8.42   \n",
      "   1    |  3340   |   0.693119   |   8.40   \n",
      "   1    |  3360   |   0.707525   |   8.40   \n",
      "   1    |  3380   |   0.698136   |   8.43   \n",
      "   1    |  3400   |   0.691240   |   8.41   \n",
      "   1    |  3420   |   0.696694   |   8.43   \n",
      "   1    |  3440   |   0.696316   |   8.42   \n",
      "   1    |  3460   |   0.693795   |   8.40   \n",
      "   1    |  3480   |   0.699348   |   8.43   \n",
      "   1    |  3500   |   0.692881   |   8.40   \n",
      "   1    |  3520   |   0.689909   |   8.39   \n",
      "   1    |  3540   |   0.689146   |   8.41   \n",
      "   1    |  3560   |   0.701051   |   8.41   \n",
      "   1    |  3580   |   0.707450   |   8.41   \n",
      "   1    |  3600   |   0.684962   |   8.40   \n",
      "   1    |  3620   |   0.710076   |   8.42   \n",
      "   1    |  3640   |   0.697999   |   8.42   \n",
      "   1    |  3660   |   0.697280   |   8.40   \n",
      "   1    |  3680   |   0.694721   |   8.38   \n",
      "   1    |  3700   |   0.694043   |   8.39   \n",
      "   1    |  3720   |   0.695279   |   8.41   \n",
      "   1    |  3740   |   0.686281   |   8.41   \n",
      "   1    |  3760   |   0.697240   |   8.42   \n",
      "   1    |  3780   |   0.684533   |   8.36   \n",
      "   1    |  3800   |   0.701316   |   8.39   \n",
      "   1    |  3820   |   0.687131   |   8.40   \n",
      "   1    |  3840   |   0.705089   |   8.43   \n",
      "   1    |  3860   |   0.695806   |   8.38   \n",
      "   1    |  3880   |   0.693776   |   8.38   \n",
      "   1    |  3900   |   0.687253   |   8.40   \n",
      "   1    |  3920   |   0.697128   |   8.38   \n",
      "   1    |  3940   |   0.687438   |   8.40   \n",
      "   1    |  3960   |   0.696320   |   8.38   \n",
      "   1    |  3980   |   0.696357   |   8.42   \n",
      "   1    |  4000   |   0.693104   |   8.43   \n",
      "   1    |  4020   |   0.700738   |   8.39   \n",
      "   1    |  4040   |   0.694496   |   8.43   \n",
      "   1    |  4060   |   0.689588   |   8.40   \n",
      "   1    |  4080   |   0.691724   |   8.43   \n",
      "   1    |  4100   |   0.693465   |   8.42   \n",
      "   1    |  4120   |   0.674705   |   8.44   \n",
      "   1    |  4140   |   0.691635   |   8.42   \n",
      "   1    |  4160   |   0.709332   |   8.43   \n",
      "   1    |  4180   |   0.698945   |   8.40   \n",
      "   1    |  4200   |   0.697316   |   8.40   \n",
      "   1    |  4220   |   0.695968   |   8.42   \n",
      "   1    |  4240   |   0.682099   |   8.41   \n",
      "   1    |  4260   |   0.703887   |   8.39   \n",
      "   1    |  4280   |   0.692589   |   8.35   \n",
      "   1    |  4300   |   0.693489   |   8.39   \n",
      "   1    |  4320   |   0.691557   |   8.41   \n",
      "   1    |  4340   |   0.696731   |   8.40   \n",
      "   1    |  4360   |   0.697711   |   8.39   \n",
      "   1    |  4380   |   0.696803   |   8.43   \n",
      "   1    |  4400   |   0.692742   |   8.40   \n",
      "   1    |  4420   |   0.694025   |   8.40   \n",
      "   1    |  4440   |   0.693572   |   8.42   \n",
      "   1    |  4460   |   0.695909   |   8.39   \n",
      "   1    |  4480   |   0.683341   |   8.41   \n",
      "   1    |  4500   |   0.703777   |   8.42   \n",
      "   1    |  4520   |   0.697226   |   8.41   \n",
      "   1    |  4540   |   0.692489   |   8.41   \n",
      "   1    |  4560   |   0.694757   |   8.41   \n",
      "   1    |  4580   |   0.700559   |   8.40   \n",
      "   1    |  4600   |   0.690764   |   8.34   \n",
      "   1    |  4620   |   0.695444   |   8.39   \n",
      "   1    |  4640   |   0.696408   |   8.42   \n",
      "   1    |  4660   |   0.692466   |   8.39   \n",
      "   1    |  4680   |   0.697650   |   8.42   \n",
      "   1    |  4700   |   0.694625   |   8.36   \n",
      "   1    |  4720   |   0.693898   |   8.40   \n",
      "   1    |  4740   |   0.695394   |   8.40   \n",
      "   1    |  4760   |   0.690787   |   8.36   \n",
      "   1    |  4780   |   0.692943   |   8.39   \n",
      "   1    |  4800   |   0.694459   |   8.42   \n",
      "   1    |  4820   |   0.694058   |   8.39   \n",
      "   1    |  4840   |   0.692347   |   8.44   \n",
      "   1    |  4860   |   0.708293   |   8.38   \n",
      "   1    |  4880   |   0.693791   |   8.41   \n",
      "   1    |  4900   |   0.694400   |   8.38   \n",
      "   1    |  4920   |   0.690642   |   8.40   \n",
      "   1    |  4940   |   0.706487   |   8.40   \n",
      "   1    |  4960   |   0.690850   |   8.37   \n",
      "   1    |  4980   |   0.694605   |   8.37   \n",
      "   1    |  4999   |   0.694649   |   7.94   \n",
      " Epoch  |  Batch  |  Train Loss  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.695325   |   8.77   \n",
      "   2    |   40    |   0.696184   |   8.41   \n",
      "   2    |   60    |   0.687659   |   8.39   \n",
      "   2    |   80    |   0.695004   |   8.37   \n",
      "   2    |   100   |   0.684127   |   8.38   \n",
      "   2    |   120   |   0.697871   |   8.39   \n",
      "   2    |   140   |   0.694619   |   8.38   \n",
      "   2    |   160   |   0.698181   |   8.38   \n",
      "   2    |   180   |   0.693790   |   8.44   \n",
      "   2    |   200   |   0.695568   |   8.38   \n",
      "   2    |   220   |   0.697685   |   8.42   \n",
      "   2    |   240   |   0.692226   |   8.42   \n",
      "   2    |   260   |   0.695056   |   8.40   \n",
      "   2    |   280   |   0.695031   |   8.39   \n",
      "   2    |   300   |   0.686812   |   8.41   \n",
      "   2    |   320   |   0.709674   |   8.36   \n",
      "   2    |   340   |   0.696388   |   8.42   \n",
      "   2    |   360   |   0.696214   |   8.41   \n",
      "   2    |   380   |   0.693675   |   8.35   \n",
      "   2    |   400   |   0.695468   |   8.38   \n",
      "   2    |   420   |   0.693729   |   8.39   \n",
      "   2    |   440   |   0.697528   |   8.39   \n",
      "   2    |   460   |   0.692829   |   8.42   \n",
      "   2    |   480   |   0.701663   |   8.40   \n",
      "   2    |   500   |   0.689245   |   8.46   \n",
      "   2    |   520   |   0.695084   |   8.40   \n",
      "   2    |   540   |   0.689373   |   8.38   \n",
      "   2    |   560   |   0.700004   |   8.41   \n",
      "   2    |   580   |   0.692930   |   8.39   \n",
      "   2    |   600   |   0.705247   |   8.44   \n",
      "   2    |   620   |   0.692800   |   8.38   \n",
      "   2    |   640   |   0.698966   |   8.40   \n",
      "   2    |   660   |   0.693366   |   8.40   \n",
      "   2    |   680   |   0.691547   |   8.39   \n",
      "   2    |   700   |   0.695577   |   8.37   \n",
      "   2    |   720   |   0.692378   |   8.39   \n",
      "   2    |   740   |   0.691479   |   8.40   \n",
      "   2    |   760   |   0.699801   |   8.36   \n",
      "   2    |   780   |   0.690372   |   8.41   \n",
      "   2    |   800   |   0.684999   |   8.39   \n",
      "   2    |   820   |   0.695443   |   8.40   \n",
      "   2    |   840   |   0.707467   |   8.42   \n",
      "   2    |   860   |   0.696564   |   8.43   \n",
      "   2    |   880   |   0.687763   |   8.40   \n",
      "   2    |   900   |   0.677437   |   8.39   \n",
      "   2    |   920   |   0.723430   |   8.38   \n",
      "   2    |   940   |   0.689419   |   8.38   \n",
      "   2    |   960   |   0.694207   |   8.40   \n",
      "   2    |   980   |   0.689833   |   8.41   \n",
      "   2    |  1000   |   0.695747   |   8.40   \n",
      "   2    |  1020   |   0.699302   |   8.45   \n",
      "   2    |  1040   |   0.692536   |   8.41   \n",
      "   2    |  1060   |   0.700143   |   8.40   \n",
      "   2    |  1080   |   0.693588   |   8.37   \n",
      "   2    |  1100   |   0.695511   |   8.41   \n",
      "   2    |  1120   |   0.695961   |   8.42   \n",
      "   2    |  1140   |   0.695062   |   8.36   \n",
      "   2    |  1160   |   0.694730   |   8.39   \n",
      "   2    |  1180   |   0.696016   |   8.39   \n",
      "   2    |  1200   |   0.695526   |   8.36   \n",
      "   2    |  1220   |   0.691127   |   8.43   \n",
      "   2    |  1240   |   0.694893   |   8.43   \n",
      "   2    |  1260   |   0.695095   |   8.40   \n",
      "   2    |  1280   |   0.697104   |   8.39   \n",
      "   2    |  1300   |   0.694671   |   8.37   \n",
      "   2    |  1320   |   0.694303   |   8.40   \n",
      "   2    |  1340   |   0.691862   |   8.42   \n",
      "   2    |  1360   |   0.691671   |   8.40   \n",
      "   2    |  1380   |   0.695768   |   8.42   \n",
      "   2    |  1400   |   0.698020   |   8.38   \n",
      "   2    |  1420   |   0.695220   |   8.39   \n",
      "   2    |  1440   |   0.686610   |   8.42   \n",
      "   2    |  1460   |   0.709659   |   8.39   \n",
      "   2    |  1480   |   0.692285   |   8.39   \n",
      "   2    |  1500   |   0.692962   |   8.39   \n",
      "   2    |  1520   |   0.696837   |   8.41   \n",
      "   2    |  1540   |   0.692343   |   8.39   \n",
      "   2    |  1560   |   0.694283   |   8.38   \n",
      "   2    |  1580   |   0.694379   |   8.38   \n",
      "   2    |  1600   |   0.692823   |   8.41   \n",
      "   2    |  1620   |   0.693795   |   8.35   \n",
      "   2    |  1640   |   0.701596   |   8.43   \n",
      "   2    |  1660   |   0.692593   |   8.40   \n",
      "   2    |  1680   |   0.691582   |   8.40   \n",
      "   2    |  1700   |   0.679769   |   8.37   \n",
      "   2    |  1720   |   0.688283   |   8.40   \n",
      "   2    |  1740   |   0.701467   |   8.42   \n",
      "   2    |  1760   |   0.696007   |   8.39   \n",
      "   2    |  1780   |   0.698390   |   8.44   \n",
      "   2    |  1800   |   0.704294   |   8.41   \n",
      "   2    |  1820   |   0.696240   |   8.44   \n",
      "   2    |  1840   |   0.694000   |   8.38   \n",
      "   2    |  1860   |   0.695835   |   8.43   \n",
      "   2    |  1880   |   0.693868   |   8.38   \n",
      "   2    |  1900   |   0.690435   |   8.42   \n",
      "   2    |  1920   |   0.701148   |   8.46   \n",
      "   2    |  1940   |   0.693112   |   8.44   \n",
      "   2    |  1960   |   0.697936   |   8.38   \n",
      "   2    |  1980   |   0.695120   |   8.41   \n",
      "   2    |  2000   |   0.690733   |   8.41   \n",
      "   2    |  2020   |   0.695894   |   8.40   \n",
      "   2    |  2040   |   0.698547   |   8.40   \n",
      "   2    |  2060   |   0.693522   |   8.40   \n",
      "   2    |  2080   |   0.692292   |   8.40   \n",
      "   2    |  2100   |   0.698150   |   8.42   \n",
      "   2    |  2120   |   0.694770   |   8.39   \n",
      "   2    |  2140   |   0.689928   |   8.42   \n",
      "   2    |  2160   |   0.686045   |   8.39   \n",
      "   2    |  2180   |   0.703846   |   8.39   \n",
      "   2    |  2200   |   0.694943   |   8.42   \n",
      "   2    |  2220   |   0.697644   |   8.42   \n",
      "   2    |  2240   |   0.694480   |   8.39   \n",
      "   2    |  2260   |   0.688899   |   8.43   \n",
      "   2    |  2280   |   0.686286   |   8.43   \n",
      "   2    |  2300   |   0.682747   |   8.42   \n",
      "   2    |  2320   |   0.703109   |   8.42   \n",
      "   2    |  2340   |   0.681157   |   8.47   \n",
      "   2    |  2360   |   0.690184   |   8.41   \n",
      "   2    |  2380   |   0.673845   |   8.40   \n",
      "   2    |  2400   |   0.711602   |   8.41   \n",
      "   2    |  2420   |   0.688874   |   8.40   \n",
      "   2    |  2440   |   0.687279   |   8.41   \n",
      "   2    |  2460   |   0.693182   |   8.40   \n",
      "   2    |  2480   |   0.797496   |   8.40   \n",
      "   2    |  2500   |   0.710548   |   8.42   \n",
      "   2    |  2520   |   0.712700   |   8.40   \n",
      "   2    |  2540   |   0.696717   |   8.43   \n",
      "   2    |  2560   |   0.700540   |   8.43   \n",
      "   2    |  2580   |   0.703853   |   8.45   \n",
      "   2    |  2600   |   0.706242   |   8.42   \n",
      "   2    |  2620   |   0.698405   |   8.45   \n",
      "   2    |  2640   |   0.696719   |   8.46   \n",
      "   2    |  2660   |   0.702137   |   8.43   \n",
      "   2    |  2680   |   0.688717   |   8.45   \n",
      "   2    |  2700   |   0.693317   |   8.44   \n",
      "   2    |  2720   |   0.694859   |   8.42   \n",
      "   2    |  2740   |   0.691674   |   8.44   \n",
      "   2    |  2760   |   0.688970   |   8.42   \n",
      "   2    |  2780   |   0.696850   |   8.45   \n",
      "   2    |  2800   |   0.701319   |   8.47   \n",
      "   2    |  2820   |   0.704645   |   8.43   \n",
      "   2    |  2840   |   0.696184   |   8.44   \n",
      "   2    |  2860   |   0.702504   |   8.44   \n",
      "   2    |  2880   |   0.695144   |   8.41   \n",
      "   2    |  2900   |   0.685143   |   8.42   \n",
      "   2    |  2920   |   0.694824   |   8.43   \n",
      "   2    |  2940   |   0.704943   |   8.42   \n",
      "   2    |  2960   |   0.688132   |   8.42   \n",
      "   2    |  2980   |   0.690607   |   8.43   \n",
      "   2    |  3000   |   0.688362   |   8.48   \n",
      "   2    |  3020   |   0.698958   |   8.41   \n",
      "   2    |  3040   |   0.690663   |   8.45   \n",
      "   2    |  3060   |   0.683923   |   8.42   \n",
      "   2    |  3080   |   0.703500   |   8.46   \n",
      "   2    |  3100   |   0.700905   |   8.42   \n",
      "   2    |  3120   |   0.703688   |   8.41   \n",
      "   2    |  3140   |   0.697365   |   8.41   \n",
      "   2    |  3160   |   0.697419   |   8.44   \n",
      "   2    |  3180   |   0.695425   |   8.39   \n",
      "   2    |  3200   |   0.691686   |   8.43   \n",
      "   2    |  3220   |   0.690400   |   8.44   \n",
      "   2    |  3240   |   0.696220   |   8.42   \n",
      "   2    |  3260   |   0.695450   |   8.43   \n",
      "   2    |  3280   |   0.698577   |   8.46   \n",
      "   2    |  3300   |   0.693178   |   8.44   \n",
      "   2    |  3320   |   0.705554   |   8.44   \n",
      "   2    |  3340   |   0.697031   |   8.42   \n",
      "   2    |  3360   |   0.704106   |   8.43   \n",
      "   2    |  3380   |   0.693308   |   8.41   \n",
      "   2    |  3400   |   0.694155   |   8.43   \n",
      "   2    |  3420   |   0.702381   |   8.43   \n",
      "   2    |  3440   |   0.687230   |   8.44   \n",
      "   2    |  3460   |   0.708981   |   8.45   \n",
      "   2    |  3480   |   0.691113   |   8.41   \n",
      "   2    |  3500   |   0.694120   |   8.45   \n",
      "   2    |  3520   |   0.699306   |   8.41   \n",
      "   2    |  3540   |   0.696659   |   8.42   \n",
      "   2    |  3560   |   0.703219   |   8.42   \n",
      "   2    |  3580   |   0.696962   |   8.42   \n",
      "   2    |  3600   |   0.683982   |   8.40   \n",
      "   2    |  3620   |   0.691201   |   8.44   \n",
      "   2    |  3640   |   0.694086   |   8.46   \n",
      "   2    |  3660   |   0.687584   |   8.42   \n",
      "   2    |  3680   |   0.699810   |   8.43   \n",
      "   2    |  3700   |   0.688911   |   8.39   \n",
      "   2    |  3720   |   0.701541   |   8.42   \n",
      "   2    |  3740   |   0.703110   |   8.42   \n",
      "   2    |  3760   |   0.697922   |   8.45   \n",
      "   2    |  3780   |   0.704547   |   8.45   \n",
      "   2    |  3800   |   0.693642   |   8.42   \n",
      "   2    |  3820   |   0.697392   |   8.42   \n",
      "   2    |  3840   |   0.692415   |   8.43   \n",
      "   2    |  3860   |   0.690752   |   8.43   \n",
      "   2    |  3880   |   0.682101   |   8.42   \n",
      "   2    |  3900   |   0.700768   |   8.45   \n",
      "   2    |  3920   |   0.696943   |   8.44   \n",
      "   2    |  3940   |   0.697339   |   8.43   \n",
      "   2    |  3960   |   0.690952   |   8.44   \n",
      "   2    |  3980   |   0.688975   |   8.42   \n",
      "   2    |  4000   |   0.699594   |   8.37   \n",
      "   2    |  4020   |   0.696366   |   8.40   \n",
      "   2    |  4040   |   0.691311   |   8.42   \n",
      "   2    |  4060   |   0.702138   |   8.41   \n",
      "   2    |  4080   |   0.696612   |   8.41   \n",
      "   2    |  4100   |   0.686225   |   8.42   \n",
      "   2    |  4120   |   0.691239   |   8.42   \n",
      "   2    |  4140   |   0.696915   |   8.41   \n",
      "   2    |  4160   |   0.704754   |   8.42   \n",
      "   2    |  4180   |   0.702658   |   8.45   \n",
      "   2    |  4200   |   0.685561   |   8.44   \n",
      "   2    |  4220   |   0.689475   |   8.47   \n",
      "   2    |  4240   |   0.693755   |   8.45   \n",
      "   2    |  4260   |   0.698026   |   8.44   \n",
      "   2    |  4280   |   0.681340   |   8.40   \n",
      "   2    |  4300   |   0.699398   |   8.39   \n",
      "   2    |  4320   |   0.697467   |   8.37   \n",
      "   2    |  4340   |   0.697198   |   8.42   \n",
      "   2    |  4360   |   0.695489   |   8.42   \n",
      "   2    |  4380   |   0.695027   |   8.44   \n",
      "   2    |  4400   |   0.691134   |   8.40   \n",
      "   2    |  4420   |   0.694636   |   8.42   \n",
      "   2    |  4440   |   0.693390   |   8.40   \n",
      "   2    |  4460   |   0.694774   |   8.45   \n",
      "   2    |  4480   |   0.692713   |   8.42   \n",
      "   2    |  4500   |   0.694871   |   8.42   \n",
      "   2    |  4520   |   0.696969   |   8.42   \n",
      "   2    |  4540   |   0.693093   |   8.41   \n",
      "   2    |  4560   |   0.688729   |   8.37   \n",
      "   2    |  4580   |   0.693974   |   8.42   \n",
      "   2    |  4600   |   0.694151   |   8.41   \n",
      "   2    |  4620   |   0.693867   |   8.38   \n",
      "   2    |  4640   |   0.691233   |   8.40   \n",
      "   2    |  4660   |   0.695507   |   8.39   \n",
      "   2    |  4680   |   0.692571   |   8.42   \n",
      "   2    |  4700   |   0.688440   |   8.36   \n",
      "   2    |  4720   |   0.694530   |   8.42   \n",
      "   2    |  4740   |   0.696601   |   8.37   \n",
      "   2    |  4760   |   0.697181   |   8.41   \n",
      "   2    |  4780   |   0.691877   |   8.41   \n",
      "   2    |  4800   |   0.692956   |   8.39   \n",
      "   2    |  4820   |   0.688797   |   8.40   \n",
      "   2    |  4840   |   0.691349   |   8.38   \n",
      "   2    |  4860   |   0.690502   |   8.37   \n",
      "   2    |  4880   |   0.693484   |   8.40   \n",
      "   2    |  4900   |   0.691912   |   8.39   \n",
      "   2    |  4920   |   0.690372   |   8.42   \n",
      "   2    |  4940   |   0.689751   |   8.42   \n",
      "   2    |  4960   |   0.688893   |   8.39   \n",
      "   2    |  4980   |   0.696711   |   8.42   \n",
      "   2    |  4999   |   0.691945   |   7.94   \n",
      "CPU times: user 38min 7s, sys: 31min 53s, total: 1h 10min 1s\n",
      "Wall time: 1h 10min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "model,avg_train_loss = train(bert_classifier, train_loader, val_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "06yCOzrdd7Wa"
   },
   "outputs": [],
   "source": [
    "val_loss, val_accuracy,F1_score = evaluate(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnyAE4Byed_k",
    "outputId": "724e80ac-1ac4-481b-a635-4db8f34bdddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss is 0.6931706943035125, accuracy is 50.34 and F1 score is 0.6332190476190476\n"
     ]
    }
   ],
   "source": [
    "#Validation results\n",
    "print(f\"The validation loss is {val_loss}, accuracy is {val_accuracy} and F1 score is {F1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvPDPumRvDPu"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/runs_final/BERT_IMDB"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT IMDB Classifier final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1fbfecf68dba4c25ab0a2d0b73d5170c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21e79ac10033433ca9ba0b34a3b5abb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d77dcf0be0914312909836ea39070a10",
      "placeholder": "​",
      "style": "IPY_MODEL_72a3774e91ac464ba3b205f3fa4cb787",
      "value": " 232k/232k [00:00&lt;00:00, 778kB/s]"
     }
    },
    "321e150fa1864f5082f49ffd382a4a30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3281b1e48c5b4169818a988778aae2b8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "329c33acd8d344e3abe99ffa585e41d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32b44b2e83b0409187257b8986b7434f",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b512d12fad44f638ec88056aa5c560a",
      "value": 466062
     }
    },
    "32b44b2e83b0409187257b8986b7434f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47ae59bf5ebc499fb0d0b2ee482fd127": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "501824dd790a44349b8becc22a8f9126": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa534f9a353a4d09a1f05bf40add720c",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_740db8cf72894bbdb6ceea132672281f",
      "value": 28
     }
    },
    "56a9b47951004cd5aa32dd7f9e390ddc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72a3774e91ac464ba3b205f3fa4cb787": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "740db8cf72894bbdb6ceea132672281f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7f099d82a32e4b90bc154f852c9d91c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c40a135259948448f599851ee090bf3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9780853cf5744ce18de54c277f59d070": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f099d82a32e4b90bc154f852c9d91c0",
      "placeholder": "​",
      "style": "IPY_MODEL_b631627a93154b7c8665c2f2fad027d8",
      "value": " 28.0/28.0 [00:00&lt;00:00, 168B/s]"
     }
    },
    "9b512d12fad44f638ec88056aa5c560a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9b590b60be3b4e95ac131de65c8d7fcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bda1bf493f1d4e83aa3370a3bf669e23",
       "IPY_MODEL_21e79ac10033433ca9ba0b34a3b5abb9"
      ],
      "layout": "IPY_MODEL_47ae59bf5ebc499fb0d0b2ee482fd127"
     }
    },
    "a6507069ab6d4c799102510221b871dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_329c33acd8d344e3abe99ffa585e41d2",
       "IPY_MODEL_ad2c659fc02846e2ae62913990e8d859"
      ],
      "layout": "IPY_MODEL_56a9b47951004cd5aa32dd7f9e390ddc"
     }
    },
    "ac68c99a46db410bbd1d3fb0843b508f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad2c659fc02846e2ae62913990e8d859": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c40a135259948448f599851ee090bf3",
      "placeholder": "​",
      "style": "IPY_MODEL_1fbfecf68dba4c25ab0a2d0b73d5170c",
      "value": " 466k/466k [00:00&lt;00:00, 5.89MB/s]"
     }
    },
    "b631627a93154b7c8665c2f2fad027d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bda1bf493f1d4e83aa3370a3bf669e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac68c99a46db410bbd1d3fb0843b508f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_321e150fa1864f5082f49ffd382a4a30",
      "value": 231508
     }
    },
    "d52d875d83c54a94af6f768a571f494a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_501824dd790a44349b8becc22a8f9126",
       "IPY_MODEL_9780853cf5744ce18de54c277f59d070"
      ],
      "layout": "IPY_MODEL_3281b1e48c5b4169818a988778aae2b8"
     }
    },
    "d77dcf0be0914312909836ea39070a10": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa534f9a353a4d09a1f05bf40add720c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
